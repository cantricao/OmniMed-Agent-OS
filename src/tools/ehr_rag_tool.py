import os
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.tools import tool

# =====================================================================
# CONFIGURATION
# =====================================================================
# Using the Vietnamese BI-Encoder optimized for native language understanding.
# Developed by Hanoi University of Science and Technology (BKAI).
EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"
CHROMA_DB_DIR = "./data/vietnamese_med_corpus/chroma_db"

def get_vietnamese_vector_db() -> Chroma:
    """
    Initializes the embedding model and connects to the local Chroma database.
    This function forces the embedding model to load onto the GPU (CUDA) 
    if available, ensuring fast semantic search operations without crashing the RAM.
    """
    # 1. Initialize the HuggingFace Embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'}, # Offload to T4 GPU
        encode_kwargs={'normalize_embeddings': True} # Required for cosine similarity
    )
    
    # 2. Connect to the local ChromaDB instance
    db = Chroma(
        collection_name="vietnamese_ehr_records",
        embedding_function=embeddings,
        persist_directory=CHROMA_DB_DIR
    )
    return db

@tool
def search_patient_records(query: str) -> str:
    """
    Use this tool to search for patient medical records (EHR), clinical notes, 
    allergies, and medical history from the localized Vietnamese Vector Database.
    
    Args:
        query (str): The search query or keywords generated by the reasoning LLM.
        
    Returns:
        str: A concatenated string of the most relevant medical documents.
    """
    try:
        print(f"üîç [RAG Node] Executing semantic search for query: '{query}'...")
        
        # Instantiate the database connection
        db = get_vietnamese_vector_db()
        
        # Perform similarity search, fetching the top 3 most relevant context chunks
        docs = db.similarity_search(query, k=3)
        
        # Handle cases where the database might be empty or no matches are found
        if not docs:
            return "WARNING: No relevant medical data found in the patient records database."
            
        # Combine the retrieved contexts into a single, structured markdown block
        # This formatting helps the downstream reasoning LLM digest the information easily
        context = "\n\n--- RETRIEVED MEDICAL CONTEXT ---\n\n".join(
            [doc.page_content for doc in docs]
        )
        
        print("‚úÖ [RAG Node] Successfully retrieved relevant medical history.")
        return f"Retrieved Context from Vietnamese EHR Database:\n\n{context}"
        
    except Exception as e:
        # Fallback error handling to prevent the entire Agent graph from crashing
        error_msg = f"CRITICAL ERROR retrieving RAG data: {str(e)}"
        print(f"‚ùå {error_msg}")
        return error_msg

# =====================================================================
# LOCAL TESTING BLOCK (Comment out in production)
# =====================================================================
# if __name__ == "__main__":
#     test_query = "B·ªánh nh√¢n Nguy·ªÖn VƒÉn A c√≥ ti·ªÅn s·ª≠ d·ªã ·ª©ng thu·ªëc g√¨?"
#     result = search_patient_records.invoke(test_query)
#     print(result)
