import os
from typing import TypedDict, Optional
from langgraph.graph import StateGraph, END

# Import our custom multimodal tools and local reasoning engine
from src.tools.ocr_vision_tool import extract_medical_document_ocr
from src.tools.ehr_rag_tool import search_patient_records
from src.tools.voice_tts_tool import generate_clinical_voice_alert
from src.core.local_llm import invoke_clinical_reasoning

# =====================================================================
# 1. DEFINE THE GRAPH STATE
# =====================================================================
# This dictionary carries the data through the pipeline from node to node.
class MedicalState(TypedDict):
    doctor_query: str
    patient_id: Optional[str]
    document_path: Optional[str]
    
    # Intermediate data generated by the nodes
    ocr_extracted_text: Optional[str]
    rag_clinical_context: Optional[str]
    final_diagnosis: Optional[str]
    voice_alert_path: Optional[str]

# =====================================================================
# 2. DEFINE THE PROCESSING NODES
# =====================================================================

def vision_node(state: MedicalState) -> MedicalState:
    """Processes medical documents or test results using Docling OCR."""
    print("\n‚ñ∂Ô∏è [STEP 1] EXECUTING VISION NODE...")
    if state.get("document_path"):
        result = extract_medical_document_ocr.invoke(state["document_path"])
        state["ocr_extracted_text"] = result
    else:
        print("‚è≠Ô∏è No document provided. Skipping Vision Node.")
        state["ocr_extracted_text"] = "No external medical documents provided."
    return state

def rag_node(state: MedicalState) -> MedicalState:
    """Retrieves patient history from the local ChromaDB (Vietnamese EHR)."""
    print("\n‚ñ∂Ô∏è [STEP 2] EXECUTING RAG NODE...")
    # Search the vector database using the doctor's initial query
    result = search_patient_records.invoke(state["doctor_query"])
    state["rag_clinical_context"] = result
    return state

def reasoning_node(state: MedicalState) -> MedicalState:
    """Loads Llama-3 to analyze OCR and RAG data, then purges VRAM."""
    print("\n‚ñ∂Ô∏è [STEP 3] EXECUTING CLINICAL REASONING NODE...")
    
    # Combine the data for the LLM context
    combined_context = f"--- RAG EHR DATA ---\n{state.get('rag_clinical_context', '')}\n\n"
    combined_context += f"--- OCR DOCUMENT DATA ---\n{state.get('ocr_extracted_text', '')}"
    
    # Invoke the dynamically loaded Llama-3 model
    diagnosis = invoke_clinical_reasoning(combined_context, state["doctor_query"])
    state["final_diagnosis"] = diagnosis
    return state

def voice_node(state: MedicalState) -> MedicalState:
    """Generates an audio alert for the doctor using local VoxCPM."""
    print("\n‚ñ∂Ô∏è [STEP 4] EXECUTING VOICE ALERT NODE...")
    if state.get("final_diagnosis"):
        # Create a concise version of the diagnosis for the voice alert
        voice_prompt = state["final_diagnosis"][:200] + "... (Please check the screen for full details)."
        alert_path = generate_clinical_voice_alert.invoke(voice_prompt)
        state["voice_alert_path"] = alert_path
    return state

# =====================================================================
# 3. BUILD AND COMPILE THE LANGGRAPH WORKFLOW
# =====================================================================
print("üèóÔ∏è Building the OmniMed-Agent-OS LangGraph pipeline...")
workflow = StateGraph(MedicalState)

# Add nodes to the graph
workflow.add_node("vision_processing", vision_node)
workflow.add_node("ehr_retrieval", rag_node)
workflow.add_node("clinical_reasoning", reasoning_node)
workflow.add_node("voice_alert", voice_node)

# Define the strict deterministic control flow (Sequential)
workflow.set_entry_point("vision_processing")
workflow.add_edge("vision_processing", "ehr_retrieval")
workflow.add_edge("ehr_retrieval", "clinical_reasoning")
workflow.add_edge("clinical_reasoning", "voice_alert")
workflow.add_edge("voice_alert", END)

# Compile the graph into a runnable application
omnimed_app = workflow.compile()

# =====================================================================
# 4. RUNNABLE DEMO / CLI INTERFACE
# =====================================================================
if __name__ == "__main__":
    print("\n" + "="*50)
    print("üè• OMNIMED-AGENT-OS: INITIALIZATION COMPLETE")
    print("="*50)
    
    # Mock input for testing the pipeline
    test_state = {
        "doctor_query": "B·ªánh nh√¢n c√≥ bi·ªÉu hi·ªán ho khan, h√£y ki·ªÉm tra k·∫øt qu·∫£ m√°u v√† ti·ªÅn s·ª≠ d·ªã ·ª©ng ƒë·ªÉ xem c√≥ n√™n d√πng kh√°ng sinh kh√¥ng.",
        "patient_id": "BN_001",
        "document_path": "data/images/sample_blood_test.pdf" # Make sure this file exists or it will skip gracefully
    }
    
    print(f"üë®‚Äç‚öïÔ∏è DOCTOR's QUERY: {test_state['doctor_query']}")
    print(f"üìÑ DOCUMENT ATTACHED: {test_state['document_path']}\n")
    
    # Execute the graph
    final_state = omnimed_app.invoke(test_state)
    
    print("\n" + "="*50)
    print("üìã OMNIMED FINAL CLINICAL REPORT")
    print("="*50)
    print(final_state["final_diagnosis"])
    print("\nüîä AUDIO ALERT STATUS:", final_state.get("voice_alert_path", "No audio generated."))
